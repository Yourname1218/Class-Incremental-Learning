# coding=utf-8
"""
AdaBoost Ensemble Learning for Continual Learning Models
ä½¿ç”¨11å€‹é è¨“ç·´çš„æŒçºŒå­¸ç¿’æ¨¡å‹å¯¦ç¾AdaBoosté›†æˆå­¸ç¿’

åŸç†èªªæ˜ï¼š
1. å°‡11å€‹é è¨“ç·´æ¨¡å‹ä½œç‚ºå¼±å­¸ç¿’å™¨ï¼ˆweak learnersï¼‰
2. åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¯å€‹æ¨¡å‹çš„æ€§èƒ½ï¼Œè¨ˆç®—éŒ¯èª¤ç‡
3. æ ¹æ“šAdaBoostç®—æ³•è¨ˆç®—æ¯å€‹æ¨¡å‹çš„æ¬Šé‡
4. ä½¿ç”¨åŠ æ¬ŠæŠ•ç¥¨é€²è¡Œæœ€çµ‚é æ¸¬
5. æ”¯æ´å‹•æ…‹æ¨¡å‹é¸æ“‡å’Œæ¬Šé‡èª¿æ•´
"""

from __future__ import absolute_import, print_function
import argparse
import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import json
from datetime import datetime
import torchvision.transforms as transforms
from ImageFolder import ImageFolder
from CIFAR100 import CIFAR100
from models.resnet import ResNet_ImageNet, ResNet_Cifar, Generator, Discriminator, BasicBlock, Bottleneck, ClassifierMLP, ModelCNN
from torch.serialization import add_safe_globals

# è¨»å†Šå®‰å…¨çš„å…¨å±€é¡
add_safe_globals([ResNet_ImageNet, ResNet_Cifar, Generator, Discriminator, BasicBlock, Bottleneck, ClassifierMLP, ModelCNN])

class ContinualAdaBoost:
    """
    æŒçºŒå­¸ç¿’çš„AdaBoosté›†æˆå™¨
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ä½¿ç”¨å¤šå€‹å°ˆé–€çš„æ¨¡å‹ä½œç‚ºå¼±å­¸ç¿’å™¨
    2. æ¯å€‹æ¨¡å‹å°ˆæ³¨æ–¼ç‰¹å®šçš„é¡åˆ¥ç¯„åœ
    3. ä½¿ç”¨AdaBoostç®—æ³•è¨ˆç®—æ¨¡å‹æ¬Šé‡
    4. æ”¯æ´å‹•æ…‹æ¬Šé‡èª¿æ•´å’Œæ¨¡å‹é¸æ“‡
    """
    
    def __init__(self, model_paths, class_ranges, num_classes=1000, device='cuda'):
        """
        åˆå§‹åŒ–AdaBoosté›†æˆå™¨
        
        Args:
            model_paths: 11å€‹æ¨¡å‹æª”æ¡ˆçš„è·¯å¾‘åˆ—è¡¨
            class_ranges: æ¯å€‹æ¨¡å‹è² è²¬çš„é¡åˆ¥ç¯„åœ
            num_classes: ç¸½é¡åˆ¥æ•¸
            device: è¨ˆç®—è¨­å‚™
        """
        self.model_paths = model_paths
        self.class_ranges = class_ranges
        self.num_classes = num_classes
        self.device = device
        
        # æ¨¡å‹å’Œæ¬Šé‡
        self.models = []
        self.model_weights = []
        self.model_errors = []
        
        # æ€§èƒ½è¨˜éŒ„
        self.performance_history = {
            'individual_accuracies': [],
            'ensemble_accuracy': [],
            'model_weights': [],
            'class_wise_performance': {}
        }
        
        # è¼‰å…¥æ‰€æœ‰æ¨¡å‹
        self._load_models()
        
    def _load_models(self):
        """å®‰å…¨è¼‰å…¥æ‰€æœ‰æ¨¡å‹"""
        print("æ­£åœ¨è¼‰å…¥11å€‹åŸºç¤æ¨¡å‹...")
        
        for i, (model_path, class_range) in enumerate(zip(self.model_paths, self.class_ranges)):
            try:
                print(f"è¼‰å…¥æ¨¡å‹ {i+1}/11: {os.path.basename(model_path)}")
                model = self._safe_load_model(model_path)
                model = model.to(self.device)
                model.eval()
                self.models.append(model)
                print(f"  - è² è²¬é¡åˆ¥ç¯„åœ: {class_range[0]}-{class_range[1]}")
            except Exception as e:
                print(f"è¼‰å…¥æ¨¡å‹å¤±æ•— {model_path}: {e}")
                raise
        
        print(f"æˆåŠŸè¼‰å…¥ {len(self.models)} å€‹æ¨¡å‹")
        
    def _safe_load_model(self, model_path):
        """å®‰å…¨è¼‰å…¥å–®å€‹æ¨¡å‹"""
        try:
            return torch.load(model_path, weights_only=False)
        except Exception as e1:
            try:
                with torch.serialization.safe_globals([ResNet_ImageNet, ResNet_Cifar, Generator, 
                                                     Discriminator, BasicBlock, Bottleneck, 
                                                     ClassifierMLP, ModelCNN]):
                    return torch.load(model_path, weights_only=True)
            except Exception as e2:
                return torch.load(model_path, weights_only=False, map_location='cpu')
    
    def compute_model_weights(self, val_loader, method='adaboost'):
        """
        è¨ˆç®—æ¯å€‹æ¨¡å‹çš„AdaBoostæ¬Šé‡
        
        Args:
            val_loader: é©—è­‰è³‡æ–™è¼‰å…¥å™¨
            method: æ¬Šé‡è¨ˆç®—æ–¹æ³• ('adaboost', 'accuracy_based', 'confidence_based')
        """
        print("é–‹å§‹è¨ˆç®—æ¨¡å‹æ¬Šé‡...")
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœ
        all_predictions = []
        all_confidences = []
        true_labels = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                batch_predictions = []
                batch_confidences = []
                
                # ç²å–æ¯å€‹æ¨¡å‹çš„é æ¸¬
                for model_idx, model in enumerate(self.models):
                    features = model(data)
                    outputs = model.embed(features)
                    probabilities = F.softmax(outputs, dim=1)
                    
                    # ç²å–é æ¸¬é¡åˆ¥å’Œä¿¡å¿ƒåº¦
                    max_probs, predictions = torch.max(probabilities, 1)
                    
                    batch_predictions.append(predictions.cpu().numpy())
                    batch_confidences.append(max_probs.cpu().numpy())
                
                all_predictions.append(np.array(batch_predictions))
                all_confidences.append(np.array(batch_confidences))
                true_labels.append(target.cpu().numpy())
                
                if batch_idx % 10 == 0:
                    print(f"è™•ç†æ‰¹æ¬¡ {batch_idx+1}/{len(val_loader)}")
        
        # æ•´ç†è³‡æ–™æ ¼å¼
        all_predictions = np.concatenate(all_predictions, axis=1)  # (11, total_samples)
        all_confidences = np.concatenate(all_confidences, axis=1)  # (11, total_samples)
        true_labels = np.concatenate(true_labels)  # (total_samples,)
        
        # è¨ˆç®—æ¯å€‹æ¨¡å‹çš„éŒ¯èª¤ç‡å’Œæ¬Šé‡
        self.model_errors = []
        self.model_weights = []
        
        for model_idx in range(len(self.models)):
            predictions = all_predictions[model_idx]
            class_range = self.class_ranges[model_idx]
            
            # åªåœ¨è©²æ¨¡å‹çš„å°ˆæ¥­é¡åˆ¥ç¯„åœå…§è¨ˆç®—éŒ¯èª¤ç‡
            mask = (true_labels >= class_range[0]) & (true_labels <= class_range[1])
            
            if np.any(mask):
                # åœ¨å°ˆæ¥­ç¯„åœå…§çš„æ¨£æœ¬
                specialist_predictions = predictions[mask]
                specialist_labels = true_labels[mask]
                
                if method == 'adaboost':
                    # åœ¨å°ˆæ¥­ç¯„åœå…§è¨ˆç®—éŒ¯èª¤ç‡
                    error_rate = np.mean(specialist_predictions != specialist_labels)
                    error_rate = max(error_rate, 1e-10)  # é¿å…é™¤é›¶
                    error_rate = min(error_rate, 0.999)  # é¿å…æ¬Šé‡éå¤§
                    
                    # AdaBoostæ¬Šé‡è¨ˆç®—ï¼ˆåŸºæ–¼å°ˆæ¥­ç¯„åœå…§çš„è¡¨ç¾ï¼‰
                    if error_rate < 0.5:
                        weight = 0.5 * np.log((1 - error_rate) / error_rate)
                    else:
                        weight = 1e-6  # çµ¦æ¥µå°æ¬Šé‡è€Œé0
                        
                elif method == 'accuracy_based':
                    # åŸºæ–¼å°ˆæ¥­ç¯„åœå…§çš„æº–ç¢ºç‡
                    accuracy = np.mean(specialist_predictions == specialist_labels)
                    weight = accuracy ** 2
                    
                elif method == 'confidence_based':
                    # åŸºæ–¼å°ˆæ¥­ç¯„åœå…§çš„ä¿¡å¿ƒåº¦
                    specialist_confidences = all_confidences[model_idx][mask]
                    correct_mask = (specialist_predictions == specialist_labels)
                    avg_confidence = np.mean(specialist_confidences[correct_mask]) if np.any(correct_mask) else 1e-6
                    weight = avg_confidence
                    
                # æ ¹æ“šå°ˆæ¥­ç¯„åœçš„æ¨£æœ¬æ•¸é‡èª¿æ•´æ¬Šé‡
                range_sample_ratio = np.sum(mask) / len(true_labels)
                adjusted_weight = weight * (range_sample_ratio ** 0.5)  # å¹³æ–¹æ ¹èª¿æ•´ï¼Œé¿å…éåº¦æ‡²ç½°
                
            else:
                # æ²’æœ‰å°ˆæ¥­ç¯„åœå…§çš„æ¨£æœ¬ï¼Œçµ¦äºˆæœ€å°æ¬Šé‡
                error_rate = 0.999
                adjusted_weight = 1e-6
            
            # ç‚ºäº†è¨˜éŒ„ï¼Œè¨ˆç®—å…¨å±€éŒ¯èª¤ç‡
            global_error_rate = np.mean(predictions != true_labels)
            
            self.model_errors.append(global_error_rate)
            self.model_weights.append(adjusted_weight)
        
        # æ­£è¦åŒ–æ¬Šé‡
        total_weight = sum(self.model_weights)
        if total_weight > 1e-6:
            self.model_weights = [w / total_weight for w in self.model_weights]
        else:
            # å¦‚æœæ‰€æœ‰æ¬Šé‡éƒ½æ¥µå°ï¼Œä½¿ç”¨åŸºæ–¼æ¨£æœ¬æ•¸é‡çš„æ¬Šé‡
            range_weights = []
            total_samples = len(true_labels)
            for class_range in self.class_ranges:
                mask = (true_labels >= class_range[0]) & (true_labels <= class_range[1])
                range_ratio = np.sum(mask) / total_samples
                range_weights.append(max(range_ratio, 1e-6))
            
            total_range_weight = sum(range_weights)
            self.model_weights = [w / total_range_weight for w in range_weights]
        
        # é¡¯ç¤ºçµæœ
        print("\næ¨¡å‹æ¬Šé‡è¨ˆç®—å®Œæˆï¼š")
        for i, (error, weight) in enumerate(zip(self.model_errors, self.model_weights)):
            class_range = self.class_ranges[i]
            print(f"æ¨¡å‹ {i+1:2d} (é¡åˆ¥ {class_range[0]:3d}-{class_range[1]:3d}): "
                  f"éŒ¯èª¤ç‡={error:.4f}, æ¬Šé‡={weight:.4f}")
        
        return self.model_weights
    
    def predict_ensemble(self, data_loader, strategy='weighted_voting'):
        """
        ä½¿ç”¨é›†æˆæ–¹æ³•é€²è¡Œé æ¸¬
        
        Args:
            data_loader: æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨
            strategy: é›†æˆç­–ç•¥ ('weighted_voting', 'adaptive_selection', 'confidence_voting')
        """
        print(f"ä½¿ç”¨ {strategy} ç­–ç•¥é€²è¡Œé›†æˆé æ¸¬...")
        
        all_predictions = []
        all_true_labels = []
        all_individual_preds = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(data_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # ç²å–æ¯å€‹æ¨¡å‹çš„é æ¸¬
                model_predictions = []
                model_confidences = []
                
                for model in self.models:
                    features = model(data)
                    outputs = model.embed(features)
                    probabilities = F.softmax(outputs, dim=1)
                    
                    model_predictions.append(probabilities.cpu().numpy())
                    model_confidences.append(torch.max(probabilities, 1)[0].cpu().numpy())
                
                # æ ¹æ“šç­–ç•¥é€²è¡Œé›†æˆ
                if strategy == 'weighted_voting':
                    ensemble_pred = self._weighted_voting(model_predictions)
                elif strategy == 'adaptive_selection':
                    ensemble_pred = self._adaptive_selection(model_predictions, model_confidences, target.cpu().numpy())
                elif strategy == 'confidence_voting':
                    ensemble_pred = self._confidence_voting(model_predictions, model_confidences)
                else:
                    raise ValueError(f"æœªçŸ¥çš„é›†æˆç­–ç•¥: {strategy}")
                
                all_predictions.extend(ensemble_pred)
                all_true_labels.extend(target.cpu().numpy())
                
                # è¨˜éŒ„å€‹åˆ¥æ¨¡å‹é æ¸¬
                individual_preds = []
                for model_pred in model_predictions:
                    individual_preds.append(np.argmax(model_pred, axis=1))
                all_individual_preds.append(np.array(individual_preds).T)
                
                if batch_idx % 20 == 0:
                    print(f"é æ¸¬é€²åº¦: {batch_idx+1}/{len(data_loader)}")
        
        # è¨ˆç®—çµæœ
        all_individual_preds = np.concatenate(all_individual_preds, axis=0)
        accuracy = accuracy_score(all_true_labels, all_predictions)
        
        print(f"\né›†æˆé æ¸¬å®Œæˆï¼")
        print(f"é›†æˆæº–ç¢ºç‡: {accuracy:.4f}")
        
        # è¨ˆç®—å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡
        individual_accs = []
        for i in range(len(self.models)):
            ind_acc = accuracy_score(all_true_labels, all_individual_preds[:, i])
            individual_accs.append(ind_acc)
            print(f"æ¨¡å‹ {i+1:2d} æº–ç¢ºç‡: {ind_acc:.4f}")
        
        return {
            'ensemble_predictions': all_predictions,
            'true_labels': all_true_labels,
            'ensemble_accuracy': accuracy,
            'individual_accuracies': individual_accs,
            'individual_predictions': all_individual_preds
        }
    
    def _weighted_voting(self, model_predictions):
        """åŠ æ¬ŠæŠ•ç¥¨ç­–ç•¥"""
        weighted_sum = np.zeros_like(model_predictions[0])
        
        for i, pred in enumerate(model_predictions):
            weighted_sum += self.model_weights[i] * pred
            
        return np.argmax(weighted_sum, axis=1)
    
    def _adaptive_selection(self, model_predictions, model_confidences, true_labels):
        """è‡ªé©æ‡‰é¸æ“‡ç­–ç•¥ï¼šæ ¹æ“šé¡åˆ¥ç¯„åœå’Œä¿¡å¿ƒåº¦é¸æ“‡æœ€ä½³æ¨¡å‹"""
        ensemble_pred = []
        
        for sample_idx in range(len(model_predictions[0])):
            sample_preds = [pred[sample_idx] for pred in model_predictions]
            sample_confs = [conf[sample_idx] for conf in model_confidences]
            
            # æ‰¾å‡ºæœ€æœ‰ä¿¡å¿ƒçš„é æ¸¬
            best_model_idx = np.argmax(sample_confs)
            predicted_class = np.argmax(sample_preds[best_model_idx])
            
            # æª¢æŸ¥æ˜¯å¦åœ¨è©²æ¨¡å‹çš„å°ˆæ¥­ç¯„åœå…§
            class_range = self.class_ranges[best_model_idx]
            if class_range[0] <= predicted_class <= class_range[1]:
                # åœ¨å°ˆæ¥­ç¯„åœå…§ï¼Œä½¿ç”¨è©²æ¨¡å‹
                ensemble_pred.append(predicted_class)
            else:
                # ä¸åœ¨å°ˆæ¥­ç¯„åœå…§ï¼Œä½¿ç”¨åŠ æ¬ŠæŠ•ç¥¨
                weighted_sum = np.zeros(self.num_classes)
                for i, pred in enumerate(sample_preds):
                    weighted_sum += self.model_weights[i] * pred
                ensemble_pred.append(np.argmax(weighted_sum))
                
        return ensemble_pred
    
    def _confidence_voting(self, model_predictions, model_confidences):
        """åŸºæ–¼ä¿¡å¿ƒåº¦çš„æŠ•ç¥¨ç­–ç•¥"""
        ensemble_pred = []
        
        for sample_idx in range(len(model_predictions[0])):
            sample_preds = [pred[sample_idx] for pred in model_predictions]
            sample_confs = [conf[sample_idx] for conf in model_confidences]
            
            # ä½¿ç”¨ä¿¡å¿ƒåº¦ä½œç‚ºæ¬Šé‡
            weighted_sum = np.zeros(self.num_classes)
            total_confidence = sum(sample_confs)
            
            if total_confidence > 0:
                for i, (pred, conf) in enumerate(zip(sample_preds, sample_confs)):
                    normalized_conf = conf / total_confidence
                    weighted_sum += normalized_conf * pred
            else:
                # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½æ²’æœ‰ä¿¡å¿ƒï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
                for pred in sample_preds:
                    weighted_sum += pred / len(sample_preds)
                    
            ensemble_pred.append(np.argmax(weighted_sum))
            
        return ensemble_pred
    
    def analyze_performance(self, results, save_dir):
        """åˆ†æå’Œè¦–è¦ºåŒ–æ€§èƒ½çµæœ"""
        print("é–‹å§‹æ€§èƒ½åˆ†æ...")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. æ•´é«”æ€§èƒ½æ¯”è¼ƒ
        self._plot_overall_performance(results, save_dir)
        
        # 2. é¡åˆ¥ç´šåˆ¥æ€§èƒ½åˆ†æ
        self._plot_class_wise_performance(results, save_dir)
        
        # 3. æ¨¡å‹æ¬Šé‡åˆ†æ
        self._plot_model_weights(save_dir)
        
        # 4. æ··æ·†çŸ©é™£
        self._plot_confusion_matrix(results, save_dir)
        
        # 5. å„²å­˜è©³ç´°å ±å‘Š
        self._save_detailed_report(results, save_dir)
        
        print(f"æ€§èƒ½åˆ†æå®Œæˆï¼Œçµæœå„²å­˜æ–¼: {save_dir}")
    
    def _plot_overall_performance(self, results, save_dir):
        """ç¹ªè£½æ•´é«”æ€§èƒ½æ¯”è¼ƒåœ–"""
        plt.figure(figsize=(12, 8))
        
        # æº–å‚™è³‡æ–™
        model_names = [f'Model {i+1}' for i in range(len(self.models))]
        individual_accs = results['individual_accuracies']
        ensemble_acc = results['ensemble_accuracy']
        
        # ç¹ªè£½å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡
        bars = plt.bar(model_names, individual_accs, alpha=0.7, color='skyblue', label='Individual Models')
        
        # ç¹ªè£½é›†æˆæ¨¡å‹æº–ç¢ºç‡
        plt.axhline(y=ensemble_acc, color='red', linestyle='--', linewidth=2, 
                   label=f'Ensemble (AdaBoost): {ensemble_acc:.4f}')
        
        # æ·»åŠ æ¬Šé‡ä¿¡æ¯åˆ°åœ–ä¸Š
        for i, (bar, weight) in enumerate(zip(bars, self.model_weights)):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'w={weight:.3f}', ha='center', va='bottom', fontsize=8)
        
        plt.xlabel('Models')
        plt.ylabel('Accuracy')
        plt.title('AdaBoost Ensemble vs Individual Models Performance')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'overall_performance.png'), dpi=300)
        plt.close()
    
    def _plot_class_wise_performance(self, results, save_dir):
        """ç¹ªè£½é¡åˆ¥ç´šåˆ¥æ€§èƒ½åˆ†æ"""
        true_labels = results['true_labels']
        ensemble_preds = results['ensemble_predictions']
        individual_preds = results['individual_predictions']
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
        class_accuracies = {}
        unique_classes = np.unique(true_labels)
        
        for cls in unique_classes:
            mask = (np.array(true_labels) == cls)
            if np.any(mask):
                # é›†æˆæº–ç¢ºç‡
                ensemble_acc = np.mean(np.array(ensemble_preds)[mask] == cls)
                class_accuracies[cls] = {'ensemble': ensemble_acc, 'individual': []}
                
                # å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡
                for i in range(len(self.models)):
                    ind_acc = np.mean(individual_preds[mask, i] == cls)
                    class_accuracies[cls]['individual'].append(ind_acc)
        
        # ç¹ªè£½å‰50å€‹é¡åˆ¥çš„è©³ç´°æ¯”è¼ƒï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
        plt.figure(figsize=(20, 10))
        
        classes_to_plot = sorted(unique_classes)[:50]
        x_pos = np.arange(len(classes_to_plot))
        
        # ç¹ªè£½é›†æˆæ¨¡å‹
        ensemble_accs = [class_accuracies[cls]['ensemble'] for cls in classes_to_plot]
        plt.plot(x_pos, ensemble_accs, 'r-', linewidth=2, label='AdaBoost Ensemble', marker='o')
        
        # ç¹ªè£½æœ€å¥½çš„å€‹åˆ¥æ¨¡å‹
        best_individual = []
        for cls in classes_to_plot:
            best_acc = max(class_accuracies[cls]['individual'])
            best_individual.append(best_acc)
        plt.plot(x_pos, best_individual, 'b--', linewidth=2, label='Best Individual Model', marker='s')
        
        plt.xlabel('Class Index')
        plt.ylabel('Accuracy')
        plt.title('Class-wise Performance Comparison (First 50 Classes)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'class_wise_performance.png'), dpi=300)
        plt.close()
        
        # æ–°å¢ï¼šç¹ªè£½æ‰€æœ‰é¡åˆ¥çš„å®Œæ•´åˆ†æ
        if len(unique_classes) > 50:
            plt.figure(figsize=(30, 12))
            
            all_classes = sorted(unique_classes)
            x_pos_all = np.arange(len(all_classes))
            
            # æ‰€æœ‰é¡åˆ¥çš„é›†æˆæº–ç¢ºç‡
            ensemble_accs_all = [class_accuracies[cls]['ensemble'] for cls in all_classes]
            plt.plot(x_pos_all, ensemble_accs_all, 'r-', linewidth=1, label='AdaBoost Ensemble', alpha=0.8)
            
            # æ‰€æœ‰é¡åˆ¥çš„æœ€ä½³å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡
            best_individual_all = []
            for cls in all_classes:
                best_acc = max(class_accuracies[cls]['individual'])
                best_individual_all.append(best_acc)
            plt.plot(x_pos_all, best_individual_all, 'b-', linewidth=1, label='Best Individual Model', alpha=0.8)
            
            plt.xlabel('Class Index')
            plt.ylabel('Accuracy')
            plt.title(f'Complete Class-wise Performance Comparison (All {len(all_classes)} Classes)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ ä»»å‹™åˆ†ç•Œç·š
            task_boundaries = [499, 549, 599, 649, 699, 749, 799, 849, 899, 949]
            for boundary in task_boundaries:
                if boundary < len(all_classes):
                    plt.axvline(x=boundary, color='gray', linestyle='--', alpha=0.5)
            
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, 'complete_class_wise_performance.png'), dpi=300)
            plt.close()
            
            # å„²å­˜è©³ç´°çš„é¡åˆ¥çµ±è¨ˆå ±å‘Š
            with open(os.path.join(save_dir, 'complete_class_analysis.txt'), 'w', encoding='utf-8') as f:
                f.write(f"å®Œæ•´é¡åˆ¥åˆ†æå ±å‘Š (å…± {len(all_classes)} å€‹é¡åˆ¥)\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"æ¸¬è©¦è³‡æ–™ä¸­å¯¦éš›åŒ…å«çš„é¡åˆ¥ç¯„åœ: {min(all_classes)} - {max(all_classes)}\n")
                f.write(f"ç¼ºå¤±çš„é¡åˆ¥æ•¸é‡: {1000 - len(all_classes)}\n\n")
                
                # æŒ‰ä»»å‹™åˆ†çµ„çµ±è¨ˆ
                task_stats = {}
                for i, (start, end) in enumerate([(0, 499)] + [(500+i*50, 549+i*50) for i in range(10)]):
                    task_classes = [cls for cls in all_classes if start <= cls <= end]
                    if task_classes:
                        task_ensemble_acc = np.mean([class_accuracies[cls]['ensemble'] for cls in task_classes])
                        task_best_acc = np.mean([max(class_accuracies[cls]['individual']) for cls in task_classes])
                        f.write(f"ä»»å‹™ {i:2d} (é¡åˆ¥ {start:3d}-{end:3d}): ")
                        f.write(f"åŒ…å« {len(task_classes):2d} å€‹é¡åˆ¥, ")
                        f.write(f"é›†æˆå¹³å‡æº–ç¢ºç‡ {task_ensemble_acc:.4f}, ")
                        f.write(f"æœ€ä½³å€‹åˆ¥å¹³å‡æº–ç¢ºç‡ {task_best_acc:.4f}\n")
    
    def _plot_model_weights(self, save_dir):
        """ç¹ªè£½æ¨¡å‹æ¬Šé‡åˆ†å¸ƒ"""
        plt.figure(figsize=(12, 6))
        
        model_names = [f'Model {i+1}\n({self.class_ranges[i][0]}-{self.class_ranges[i][1]})' 
                      for i in range(len(self.models))]
        
        bars = plt.bar(model_names, self.model_weights, color='lightgreen', alpha=0.7)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, weight, error in zip(bars, self.model_weights, self.model_errors):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{weight:.3f}\n(err:{error:.3f})', ha='center', va='bottom', fontsize=9)
        
        plt.xlabel('Models (Class Range)')
        plt.ylabel('AdaBoost Weight')
        plt.title('AdaBoost Model Weights Distribution')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'model_weights.png'), dpi=300)
        plt.close()
    
    def _plot_confusion_matrix(self, results, save_dir):
        """ç¹ªè£½æ··æ·†çŸ©é™£ï¼ˆå–æ¨£é¡¯ç¤ºï¼‰"""
        true_labels = results['true_labels']
        ensemble_preds = results['ensemble_predictions']
        
        # é¦–å…ˆç¹ªè£½å‰100å€‹é¡åˆ¥çš„æ··æ·†çŸ©é™£ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
        mask = (np.array(true_labels) < 100) & (np.array(ensemble_preds) < 100)
        if np.any(mask):
            filtered_true = np.array(true_labels)[mask]
            filtered_pred = np.array(ensemble_preds)[mask]
            
            cm = confusion_matrix(filtered_true, filtered_pred)
            
            plt.figure(figsize=(12, 10))
            sns.heatmap(cm, annot=False, cmap='Blues', fmt='d')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.title('Confusion Matrix (First 100 Classes)')
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), dpi=300)
            plt.close()
        
        # æ–°å¢ï¼šåˆ†æå®Œæ•´çš„æ··æ·†çŸ©é™£çµ±è¨ˆè³‡è¨Š
        unique_true = np.unique(true_labels)
        unique_pred = np.unique(ensemble_preds)
        
        # å„²å­˜å®Œæ•´æ··æ·†çŸ©é™£çµ±è¨ˆ
        with open(os.path.join(save_dir, 'confusion_matrix_analysis.txt'), 'w', encoding='utf-8') as f:
            f.write("å®Œæ•´æ··æ·†çŸ©é™£åˆ†æå ±å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"å¯¦éš›æ¨™ç±¤ç¯„åœ: {min(unique_true)} - {max(unique_true)} (å…± {len(unique_true)} å€‹é¡åˆ¥)\n")
            f.write(f"é æ¸¬æ¨™ç±¤ç¯„åœ: {min(unique_pred)} - {max(unique_pred)} (å…± {len(unique_pred)} å€‹é¡åˆ¥)\n")
            f.write(f"ç¸½æ¨£æœ¬æ•¸: {len(true_labels)}\n\n")
            
            # è¨ˆç®—å„é¡åˆ¥çš„é æ¸¬åˆ†ä½ˆ
            f.write("å„é¡åˆ¥é æ¸¬çµ±è¨ˆ:\n")
            f.write("-" * 30 + "\n")
            
            for cls in sorted(unique_true):
                mask = np.array(true_labels) == cls
                cls_true_count = np.sum(mask)
                cls_preds = np.array(ensemble_preds)[mask]
                cls_correct = np.sum(cls_preds == cls)
                cls_accuracy = cls_correct / cls_true_count if cls_true_count > 0 else 0
                
                f.write(f"é¡åˆ¥ {cls:3d}: çœŸå¯¦æ¨£æœ¬ {cls_true_count:3d}, æ­£ç¢ºé æ¸¬ {cls_correct:3d}, æº–ç¢ºç‡ {cls_accuracy:.4f}\n")
            
            # é æ¸¬é¡åˆ¥åˆ†ä½ˆçµ±è¨ˆ
            f.write(f"\né æ¸¬é¡åˆ¥åˆ†ä½ˆ:\n")
            f.write("-" * 30 + "\n")
            
            pred_counts = {}
            for pred in ensemble_preds:
                pred_counts[pred] = pred_counts.get(pred, 0) + 1
            
            for pred_cls in sorted(pred_counts.keys()):
                f.write(f"é æ¸¬ç‚ºé¡åˆ¥ {pred_cls:3d}: {pred_counts[pred_cls]:3d} æ¬¡\n")
            
            # å¦‚æœé¡åˆ¥æ•¸é‡ä¸å¤ªå¤§ï¼Œå¯ä»¥ç¹ªè£½å®Œæ•´çš„å°å‹æ··æ·†çŸ©é™£
            if len(unique_true) <= 200 and len(unique_pred) <= 200:
                full_cm = confusion_matrix(true_labels, ensemble_preds)
                
                plt.figure(figsize=(max(15, len(unique_true)//5), max(12, len(unique_true)//5)))
                sns.heatmap(full_cm, annot=False, cmap='Blues', fmt='d', 
                           xticklabels=False, yticklabels=False)
                plt.xlabel('Predicted Label')
                plt.ylabel('True Label')
                plt.title(f'Complete Confusion Matrix ({len(unique_true)} classes)')
                plt.tight_layout()
                plt.savefig(os.path.join(save_dir, 'complete_confusion_matrix.png'), dpi=300)
                plt.close()
                
                f.write(f"\nå·²ç”Ÿæˆå®Œæ•´æ··æ·†çŸ©é™£åœ–: complete_confusion_matrix.png\n")
    
    def _save_detailed_report(self, results, save_dir):
        """å„²å­˜è©³ç´°çš„æ€§èƒ½å ±å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'ensemble_accuracy': float(results['ensemble_accuracy']),
            'individual_accuracies': [float(acc) for acc in results['individual_accuracies']],
            'model_weights': [float(w) for w in self.model_weights],
            'model_errors': [float(e) for e in self.model_errors],
            'class_ranges': self.class_ranges,
            'improvement_over_best_individual': float(results['ensemble_accuracy'] - max(results['individual_accuracies'])),
            'average_individual_accuracy': float(np.mean(results['individual_accuracies']))
        }
        
        with open(os.path.join(save_dir, 'performance_report.json'), 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)
        
        # ç”Ÿæˆæ–‡å­—å ±å‘Š
        with open(os.path.join(save_dir, 'performance_summary.txt'), 'w', encoding='utf-8') as f:
            f.write("AdaBoost é›†æˆå­¸ç¿’æ€§èƒ½å ±å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æ¸¬è©¦æ™‚é–“: {report['timestamp']}\n")
            f.write(f"é›†æˆæº–ç¢ºç‡: {report['ensemble_accuracy']:.4f}\n")
            f.write(f"æœ€ä½³å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡: {max(report['individual_accuracies']):.4f}\n")
            f.write(f"å¹³å‡å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡: {report['average_individual_accuracy']:.4f}\n")
            f.write(f"æ”¹å–„å¹…åº¦: {report['improvement_over_best_individual']:.4f}\n\n")
            
            f.write("å„æ¨¡å‹è©³ç´°è³‡è¨Š:\n")
            f.write("-" * 30 + "\n")
            for i in range(len(self.models)):
                f.write(f"æ¨¡å‹ {i+1:2d}: é¡åˆ¥ {self.class_ranges[i][0]:3d}-{self.class_ranges[i][1]:3d}, "
                       f"æº–ç¢ºç‡ {report['individual_accuracies'][i]:.4f}, "
                       f"æ¬Šé‡ {report['model_weights'][i]:.4f}, "
                       f"éŒ¯èª¤ç‡ {report['model_errors'][i]:.4f}\n")


def create_data_loaders(args):
    """å‰µå»ºè³‡æ–™è¼‰å…¥å™¨"""
    if args.data == 'medicine':
        mean_values = [0.485, 0.456, 0.406]
        std_values = [0.229, 0.224, 0.225]
        transform = transforms.Compose([
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean_values, std=std_values)
        ])
        
        # é©—è­‰é›†
        val_dir = os.path.join('medicine_picture', 'valid')
        val_dataset = ImageFolder(val_dir, transform=transform, index=list(range(args.num_class)))
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0
        )
        
        # æ¸¬è©¦é›†
        test_dir = os.path.join('medicine_picture', 'test') if os.path.exists('medicine_picture/test') else val_dir
        test_dataset = ImageFolder(test_dir, transform=transform, index=list(range(args.num_class)))
        test_loader = torch.utils.data.DataLoader(
            test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0
        )
        
    elif 'cifar' in args.data:
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        # CIFARè³‡æ–™é›†
        val_dataset = CIFAR100(root=args.data_dir, train=False, download=True, 
                              transform=transform, index=list(range(args.num_class)))
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0
        )
        test_loader = val_loader  # åœ¨CIFARä¸­ä½¿ç”¨ç›¸åŒçš„æ¸¬è©¦é›†
        
    return val_loader, test_loader


def main():
    parser = argparse.ArgumentParser(description='AdaBoost Ensemble for Continual Learning')
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('-data', type=str, default='medicine', help='è³‡æ–™é›†é¡å‹')
    parser.add_argument('-models_dir', type=str, required=True, help='æ¨¡å‹æª”æ¡ˆç›®éŒ„')
    parser.add_argument('-num_class', type=int, default=1000, help='é¡åˆ¥ç¸½æ•¸')
    parser.add_argument('-nb_cl_fg', type=int, default=500, help='ç¬¬ä¸€çµ„é¡åˆ¥æ•¸')
    parser.add_argument('-num_task', type=int, default=10, help='å¾ŒçºŒä»»å‹™æ•¸')
    parser.add_argument('-epochs', type=int, default=200, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('-batch_size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('-gpu', type=str, default='0', help='ä½¿ç”¨çš„GPU')
    parser.add_argument('-data_dir', type=str, default='/data/datasets/', help='è³‡æ–™ç›®éŒ„')
    
    # AdaBooståƒæ•¸
    parser.add_argument('-weight_method', type=str, default='adaboost', 
                       choices=['adaboost', 'accuracy_based', 'confidence_based'],
                       help='æ¬Šé‡è¨ˆç®—æ–¹æ³•')
    parser.add_argument('-ensemble_strategy', type=str, default='weighted_voting',
                       choices=['weighted_voting', 'adaptive_selection', 'confidence_voting'],
                       help='é›†æˆç­–ç•¥')
    
    # è¼¸å‡ºåƒæ•¸
    parser.add_argument('-save_dir', type=str, default='AdaBoost_Results', help='AdaBoostçµæœå„²å­˜æ ¹ç›®éŒ„')
    
    args = parser.parse_args()
    
    # è¨­å®šè¨­å‚™
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # æ§‹å»ºæ¨¡å‹è·¯å¾‘å’Œé¡åˆ¥ç¯„åœ
    model_paths = []
    class_ranges = []
    
    # ç¬¬ä¸€å€‹æ¨¡å‹ï¼ˆä»»å‹™0ï¼Œé¡åˆ¥0-499ï¼‰
    model_paths.append(os.path.join(args.models_dir, f'task_00_{args.epochs-1}_model.pkl'))
    class_ranges.append((0, args.nb_cl_fg - 1))
    
    # å¾ŒçºŒæ¨¡å‹ï¼ˆä»»å‹™1-10ï¼Œæ¯å€‹50å€‹é¡åˆ¥ï¼‰
    num_class_per_task = (args.num_class - args.nb_cl_fg) // args.num_task
    for task_id in range(1, args.num_task + 1):
        model_paths.append(os.path.join(args.models_dir, f'task_{task_id:02d}_{args.epochs-1}_model.pkl'))
        start_class = args.nb_cl_fg + (task_id - 1) * num_class_per_task
        end_class = args.nb_cl_fg + task_id * num_class_per_task - 1
        class_ranges.append((start_class, end_class))
    
    print("æ¨¡å‹é…ç½®:")
    for i, (path, class_range) in enumerate(zip(model_paths, class_ranges)):
        print(f"æ¨¡å‹ {i+1:2d}: {os.path.basename(path)} -> é¡åˆ¥ {class_range[0]}-{class_range[1]}")
    
    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆå­˜åœ¨æ€§
    missing_models = [path for path in model_paths if not os.path.exists(path)]
    if missing_models:
        print("éŒ¯èª¤ï¼šä»¥ä¸‹æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼š")
        for path in missing_models:
            print(f"  - {path}")
        return
    
    # å‰µå»ºAdaBoosté›†æˆå™¨
    print("\nåˆå§‹åŒ–AdaBoosté›†æˆå™¨...")
    ensemble = ContinualAdaBoost(
        model_paths=model_paths,
        class_ranges=class_ranges,
        num_classes=args.num_class,
        device=device
    )
    
    # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
    print("æº–å‚™è³‡æ–™è¼‰å…¥å™¨...")
    val_loader, test_loader = create_data_loaders(args)
    
    # è¨ˆç®—æ¨¡å‹æ¬Šé‡
    print("è¨ˆç®—AdaBoostæ¬Šé‡...")
    ensemble.compute_model_weights(val_loader, method=args.weight_method)
    
    # é€²è¡Œé›†æˆé æ¸¬
    print("åŸ·è¡Œé›†æˆé æ¸¬...")
    results = ensemble.predict_ensemble(test_loader, strategy=args.ensemble_strategy)
    
    # åˆ†æå’Œå„²å­˜çµæœ - å‰µå»ºå°ˆé–€çš„AdaBoostè³‡æ–™å¤¾çµæ§‹
    print("åˆ†ææ€§èƒ½çµæœ...")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # å‰µå»ºå±¤æ¬¡åŒ–çš„è³‡æ–™å¤¾çµæ§‹
    main_save_dir = args.save_dir
    method_dir = f"AdaBoost_{args.weight_method}_{args.ensemble_strategy}"
    time_dir = f"run_{timestamp}"
    save_dir = os.path.join(main_save_dir, method_dir, time_dir)
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\nğŸ“ AdaBoostçµæœå°‡å„²å­˜æ–¼å°ˆé–€è³‡æ–™å¤¾ï¼š")
    print(f"   {save_dir}")
    print(f"   â”œâ”€â”€ performance_report.json")
    print(f"   â”œâ”€â”€ performance_summary.txt") 
    print(f"   â”œâ”€â”€ overall_performance.png")
    print(f"   â”œâ”€â”€ class_wise_performance.png")
    print(f"   â”œâ”€â”€ model_weights.png")
    print(f"   â””â”€â”€ confusion_matrix.png")
    
    ensemble.analyze_performance(results, save_dir)
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "="*60)
    print("AdaBoost é›†æˆå­¸ç¿’å®Œæˆï¼")
    print(f"é›†æˆæº–ç¢ºç‡: {results['ensemble_accuracy']:.4f}")
    print(f"æœ€ä½³å€‹åˆ¥æ¨¡å‹: {max(results['individual_accuracies']):.4f}")
    print(f"æ”¹å–„å¹…åº¦: {results['ensemble_accuracy'] - max(results['individual_accuracies']):.4f}")
    print(f"çµæœå„²å­˜æ–¼: {save_dir}")
    print("="*60)


if __name__ == '__main__':
    main() 