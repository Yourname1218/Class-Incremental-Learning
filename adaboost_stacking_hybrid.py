#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AdaBoost-Stackingæ··åˆé›†æˆå­¸ç¿’
æ¡ç”¨Stackingçš„å‰æœŸè™•ç†æ–¹å¼ï¼Œä½†ä¿æŒAdaBoostçš„æœ€çµ‚åŠ æ¬ŠæŠ•ç¥¨æ±ºç­–
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import argparse
import json
from datetime import datetime
from tqdm import tqdm
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
import matplotlib.pyplot as plt
import seaborn as sns

# å°å…¥å°ˆæ¡ˆçš„è‡ªè¨‚ImageFolderå’ŒCIFAR100
from ImageFolder import ImageFolder
from CIFAR100 import CIFAR100


class AdaBoostStackingHybrid:
    """
    AdaBoost-Stackingæ··åˆé›†æˆå™¨
    æ¡ç”¨Stackingçš„æ¨¡å‹è©•ä¼°æ–¹å¼ï¼Œä½†ä½¿ç”¨AdaBoostçš„æœ€çµ‚æ±ºç­–æ©Ÿåˆ¶
    """
    
    def __init__(self, model_paths, class_ranges, num_classes=1000, device='cuda'):
        self.model_paths = model_paths
        self.class_ranges = class_ranges
        self.num_classes = num_classes
        self.device = device
        self.models = []
        self.model_weights = []
        self.model_errors = []
        
        print(f"åˆå§‹åŒ–AdaBoost-Stackingæ··åˆé›†æˆå™¨...")
        print(f"æ¨¡å‹æ•¸é‡: {len(model_paths)}")
        print(f"é¡åˆ¥ç¸½æ•¸: {num_classes}")
        
        # è¼‰å…¥æ¨¡å‹
        self._load_models()
    
    def _load_models(self):
        """è¼‰å…¥æ‰€æœ‰åŸºç¤æ¨¡å‹ï¼ˆæ¡ç”¨Stackingæ–¹å¼ï¼‰"""
        print("è¼‰å…¥åŸºç¤æ¨¡å‹...")
        for i, path in enumerate(self.model_paths):
            try:
                model = torch.load(path, weights_only=False, map_location=self.device)
                model.to(self.device)
                model.eval()
                self.models.append(model)
                class_range = self.class_ranges[i]
                print(f"âœ… æ¨¡å‹ {i+1:2d}: è¼‰å…¥æˆåŠŸ -> å°ˆæ¥­ç¯„åœ {class_range[0]}-{class_range[1]}")
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {i+1:2d}: è¼‰å…¥å¤±æ•— - {e}")
        
        print(f"æˆåŠŸè¼‰å…¥ {len(self.models)} å€‹æ¨¡å‹")
    
    def collect_all_predictions(self, dataloader, desc="æ”¶é›†é æ¸¬"):
        """
        æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœï¼ˆæ¡ç”¨Stackingæ–¹å¼ï¼‰
        è¿”å›: (predictions_list, true_labels)
        """
        print(f"{desc}ä¸­...")
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=desc)):
                data, target = data.to(self.device), target.to(self.device)
                batch_predictions = []
                
                # ç²å–æ¯å€‹æ¨¡å‹çš„é æ¸¬æ¦‚ç‡åˆ†å¸ƒ
                for model in self.models:
                    features = model(data)
                    outputs = model.embed(features)
                    probabilities = F.softmax(outputs, dim=1)
                    batch_predictions.append(probabilities)
                
                all_predictions.append(batch_predictions)
                all_labels.append(target)
        
        # æ•´ç†æ•¸æ“šæ ¼å¼ - æ¯å€‹æ¨¡å‹çš„æ‰€æœ‰é æ¸¬
        model_predictions = []
        for i in range(len(self.models)):
            model_preds = []
            for batch_idx in range(len(all_predictions)):
                model_preds.append(all_predictions[batch_idx][i])
            model_predictions.append(torch.cat(model_preds, dim=0))
        
        # å°‡é æ¸¬ç§»åˆ°CPUä»¥ç¯€çœGPUå…§å­˜
        model_predictions = [x.cpu() for x in model_predictions]
        true_labels = torch.cat(all_labels, dim=0).cpu()
        
        return model_predictions, true_labels
    
    def compute_stacking_weights(self, val_loader):
        """
        ä½¿ç”¨Stackingæ–¹å¼è¨ˆç®—æ¨¡å‹æ¬Šé‡
        åŸºæ–¼æ•´é«”é©—è­‰é›†è¡¨ç¾ï¼Œè€Œéå°ˆæ¥­ç¯„åœé™åˆ¶
        """
        print("ä½¿ç”¨Stackingæ–¹å¼è¨ˆç®—æ¨¡å‹æ¬Šé‡...")
        
        # æ”¶é›†é©—è­‰é›†ä¸Šçš„é æ¸¬
        model_predictions, true_labels = self.collect_all_predictions(val_loader, "é©—è­‰é›†è©•ä¼°")
        
        # è¨ˆç®—æ¯å€‹æ¨¡å‹åœ¨æ•´é«”é©—è­‰é›†ä¸Šçš„æº–ç¢ºç‡
        self.model_errors = []
        individual_accuracies = []
        
        for i, predictions in enumerate(model_predictions):
            # ç²å–é æ¸¬é¡åˆ¥
            predicted_classes = torch.argmax(predictions, dim=1)
            
            # è¨ˆç®—æº–ç¢ºç‡
            accuracy = accuracy_score(true_labels.numpy(), predicted_classes.numpy())
            individual_accuracies.append(accuracy)
            
            # è¨ˆç®—éŒ¯èª¤ç‡
            error_rate = 1.0 - accuracy
            self.model_errors.append(error_rate)
            
            print(f"æ¨¡å‹ {i+1:2d}: æº–ç¢ºç‡={accuracy:.4f}, éŒ¯èª¤ç‡={error_rate:.4f}")
        
        # ä½¿ç”¨AdaBoostæ¬Šé‡å…¬å¼ï¼Œä½†åŸºæ–¼æ•´é«”è¡¨ç¾
        self.model_weights = []
        for i, error_rate in enumerate(self.model_errors):
            # é¿å…é™¤é›¶å’Œæ¬Šé‡éå¤§
            error_rate = max(error_rate, 1e-10)
            error_rate = min(error_rate, 0.999)
            
            if error_rate < 0.5:
                weight = 0.5 * np.log((1 - error_rate) / error_rate)
            else:
                weight = 1e-6  # è¡¨ç¾å·®çš„æ¨¡å‹çµ¦æ¥µå°æ¬Šé‡
            
            self.model_weights.append(weight)
        
        # æ¬Šé‡æ­£è¦åŒ–ï¼ˆç¢ºä¿ç¸½å’Œç‚º1ï¼‰
        total_weight = sum(self.model_weights)
        if total_weight > 1e-6:
            self.model_weights = [w / total_weight for w in self.model_weights]
        else:
            # å¦‚æœæ‰€æœ‰æ¬Šé‡éƒ½æ¥µå°ï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
            self.model_weights = [1.0 / len(self.models) for _ in self.models]
        
        print("\nğŸ¯ Stackingæ–¹å¼æ¬Šé‡è¨ˆç®—å®Œæˆï¼š")
        for i, (error, weight, acc) in enumerate(zip(self.model_errors, self.model_weights, individual_accuracies)):
            class_range = self.class_ranges[i]
            print(f"æ¨¡å‹ {i+1:2d} (é¡åˆ¥ {class_range[0]:3d}-{class_range[1]:3d}): "
                  f"æº–ç¢ºç‡={acc:.4f}, éŒ¯èª¤ç‡={error:.4f}, AdaBoostæ¬Šé‡={weight:.4f}")
        
        return self.model_weights
    
    def adaboost_weighted_voting(self, model_predictions):
        """
        AdaBoostæœ€çµ‚æ±ºç­–ï¼šåŠ æ¬ŠæŠ•ç¥¨
        èˆ‡Stackingä¸åŒï¼Œé€™è£¡æ˜¯æ‰€æœ‰æ¨¡å‹åƒèˆ‡æŠ•ç¥¨ï¼Œè€Œéé¸æ“‡å–®ä¸€æ¨¡å‹
        """
        # ç¢ºä¿é æ¸¬åœ¨ç›¸åŒè¨­å‚™ä¸Š
        device = model_predictions[0].device if isinstance(model_predictions[0], torch.Tensor) else 'cpu'
        
        # è½‰æ›ç‚ºnumpyé€²è¡Œè¨ˆç®—
        if isinstance(model_predictions[0], torch.Tensor):
            numpy_predictions = [pred.cpu().numpy() for pred in model_predictions]
        else:
            numpy_predictions = model_predictions
        
        # AdaBooståŠ æ¬Šæ±‚å’Œ
        weighted_sum = np.zeros_like(numpy_predictions[0])
        
        for i, pred in enumerate(numpy_predictions):
            weighted_sum += self.model_weights[i] * pred
        
        # argmaxé¸æ“‡æœ€çµ‚é¡åˆ¥
        final_predictions = np.argmax(weighted_sum, axis=1)
        
        return final_predictions, weighted_sum
    
    def predict_ensemble(self, test_loader):
        """
        é›†æˆé æ¸¬ï¼šä½¿ç”¨AdaBoostçš„åŠ æ¬ŠæŠ•ç¥¨æ±ºç­–
        """
        print("åŸ·è¡ŒAdaBooståŠ æ¬ŠæŠ•ç¥¨é æ¸¬...")
        
        # æ”¶é›†æ¸¬è©¦é›†é æ¸¬
        model_predictions, true_labels = self.collect_all_predictions(test_loader, "æ¸¬è©¦é›†é æ¸¬")
        
        # ä½¿ç”¨AdaBooståŠ æ¬ŠæŠ•ç¥¨
        ensemble_predictions, ensemble_probabilities = self.adaboost_weighted_voting(model_predictions)
        
        # è¨ˆç®—æº–ç¢ºç‡
        ensemble_accuracy = accuracy_score(true_labels.numpy(), ensemble_predictions)
        
        # è¨ˆç®—å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡
        individual_accuracies = []
        individual_predictions = []
        
        for i, predictions in enumerate(model_predictions):
            predicted_classes = torch.argmax(predictions, dim=1).numpy()
            individual_predictions.append(predicted_classes)
            acc = accuracy_score(true_labels.numpy(), predicted_classes)
            individual_accuracies.append(acc)
        
        print(f"\nğŸ¯ AdaBoosté›†æˆé æ¸¬å®Œæˆï¼")
        print(f"é›†æˆæº–ç¢ºç‡: {ensemble_accuracy:.4f}")
        print("å„æ¨¡å‹æº–ç¢ºç‡:")
        for i, acc in enumerate(individual_accuracies):
            print(f"  æ¨¡å‹ {i+1:2d}: {acc:.4f}")
        
        return {
            'ensemble_predictions': ensemble_predictions,
            'ensemble_probabilities': ensemble_probabilities,
            'true_labels': true_labels.numpy(),
            'ensemble_accuracy': ensemble_accuracy,
            'individual_accuracies': individual_accuracies,
            'individual_predictions': np.array(individual_predictions).T,
            'model_weights': self.model_weights.copy()
        }
    
    def analyze_performance(self, results, save_dir):
        """åˆ†æå’Œè¦–è¦ºåŒ–æ€§èƒ½çµæœ"""
        print("é–‹å§‹æ€§èƒ½åˆ†æ...")
        
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. æ•´é«”æ€§èƒ½æ¯”è¼ƒ
        self._plot_overall_performance(results, save_dir)
        
        # 2. æ¬Šé‡åˆ†æ
        self._plot_model_weights(save_dir)
        
        # 3. æ··æ·†çŸ©é™£ï¼ˆå‰100é¡ï¼‰
        self._plot_confusion_matrix(results, save_dir)
        
        # 4. å„²å­˜è©³ç´°å ±å‘Š
        self._save_detailed_report(results, save_dir)
        
        print(f"æ€§èƒ½åˆ†æå®Œæˆï¼Œçµæœå„²å­˜æ–¼: {save_dir}")
    
    def _plot_overall_performance(self, results, save_dir):
        """ç¹ªè£½æ•´é«”æ€§èƒ½æ¯”è¼ƒåœ–"""
        plt.figure(figsize=(12, 8))
        
        model_names = [f'Model {i+1}' for i in range(len(self.models))]
        individual_accs = results['individual_accuracies']
        ensemble_acc = results['ensemble_accuracy']
        
        # ç¹ªè£½å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡
        bars = plt.bar(model_names, individual_accs, alpha=0.7, color='skyblue', label='Individual Models')
        
        # ç¹ªè£½é›†æˆæ¨¡å‹æº–ç¢ºç‡
        plt.axhline(y=ensemble_acc, color='red', linestyle='--', linewidth=2, 
                   label=f'AdaBoost Ensemble: {ensemble_acc:.4f}')
        
        # æ·»åŠ æ¬Šé‡ä¿¡æ¯
        for i, (bar, weight) in enumerate(zip(bars, self.model_weights)):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'w={weight:.3f}', ha='center', va='bottom', fontsize=8)
        
        plt.xlabel('Models')
        plt.ylabel('Accuracy')
        plt.title('AdaBoost-Stacking Hybrid: Performance Comparison')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'overall_performance.png'), dpi=300)
        plt.close()
    
    def _plot_model_weights(self, save_dir):
        """ç¹ªè£½æ¨¡å‹æ¬Šé‡åˆ†å¸ƒ"""
        plt.figure(figsize=(12, 6))
        
        model_names = [f'Model {i+1}\n({self.class_ranges[i][0]}-{self.class_ranges[i][1]})' 
                      for i in range(len(self.models))]
        
        bars = plt.bar(model_names, self.model_weights, color='lightgreen', alpha=0.7)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, weight, error in zip(bars, self.model_weights, self.model_errors):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{weight:.3f}\n(err:{error:.3f})', ha='center', va='bottom', fontsize=9)
        
        plt.xlabel('Models (Class Range)')
        plt.ylabel('AdaBoost Weight')
        plt.title('AdaBoost-Stacking Hybrid: Model Weights Distribution')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'model_weights.png'), dpi=300)
        plt.close()
    
    def _plot_confusion_matrix(self, results, save_dir):
        """ç¹ªè£½æ··æ·†çŸ©é™£ï¼ˆå‰100é¡ï¼‰"""
        true_labels = results['true_labels']
        ensemble_preds = results['ensemble_predictions']
        
        # åªé¡¯ç¤ºå‰100å€‹é¡åˆ¥
        mask = (true_labels < 100) & (ensemble_preds < 100)
        if np.any(mask):
            filtered_true = true_labels[mask]
            filtered_pred = ensemble_preds[mask]
            
            cm = confusion_matrix(filtered_true, filtered_pred)
            
            plt.figure(figsize=(12, 10))
            sns.heatmap(cm, annot=False, cmap='Blues', fmt='d')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.title('Confusion Matrix (First 100 Classes) - AdaBoost-Stacking Hybrid')
            plt.tight_layout()
            plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), dpi=300)
            plt.close()
    
    def _save_detailed_report(self, results, save_dir):
        """å„²å­˜è©³ç´°çš„æ€§èƒ½å ±å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'method': 'AdaBoost-Stacking Hybrid',
            'ensemble_accuracy': float(results['ensemble_accuracy']),
            'individual_accuracies': [float(acc) for acc in results['individual_accuracies']],
            'model_weights': [float(w) for w in self.model_weights],
            'model_errors': [float(e) for e in self.model_errors],
            'class_ranges': self.class_ranges,
            'improvement_over_best_individual': float(results['ensemble_accuracy'] - max(results['individual_accuracies'])),
            'average_individual_accuracy': float(np.mean(results['individual_accuracies']))
        }
        
        # JSONå ±å‘Š
        with open(os.path.join(save_dir, 'performance_report.json'), 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)
        
        # æ–‡å­—å ±å‘Š
        with open(os.path.join(save_dir, 'performance_summary.txt'), 'w', encoding='utf-8') as f:
            f.write("AdaBoost-Stacking æ··åˆé›†æˆå­¸ç¿’æ€§èƒ½å ±å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write("æ–¹æ³•ç‰¹å¾µ:\n")
            f.write("- å‰æœŸè™•ç†: æ¡ç”¨Stackingæ–¹å¼ (æ•´é«”é©—è­‰é›†è©•ä¼°)\n")
            f.write("- æœ€çµ‚æ±ºç­–: æ¡ç”¨AdaBoostæ–¹å¼ (åŠ æ¬ŠæŠ•ç¥¨)\n")
            f.write("- æ¬Šé‡è¨ˆç®—: AdaBoostå…¬å¼åŸºæ–¼æ•´é«”è¡¨ç¾\n\n")
            
            f.write(f"æ¸¬è©¦æ™‚é–“: {report['timestamp']}\n")
            f.write(f"é›†æˆæº–ç¢ºç‡: {report['ensemble_accuracy']:.4f}\n")
            f.write(f"æœ€ä½³å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡: {max(report['individual_accuracies']):.4f}\n")
            f.write(f"å¹³å‡å€‹åˆ¥æ¨¡å‹æº–ç¢ºç‡: {report['average_individual_accuracy']:.4f}\n")
            f.write(f"æ”¹å–„å¹…åº¦: {report['improvement_over_best_individual']:.4f}\n\n")
            
            f.write("å„æ¨¡å‹è©³ç´°è³‡è¨Š:\n")
            f.write("-" * 40 + "\n")
            for i in range(len(self.models)):
                f.write(f"æ¨¡å‹ {i+1:2d}: é¡åˆ¥ {self.class_ranges[i][0]:3d}-{self.class_ranges[i][1]:3d}, "
                       f"æº–ç¢ºç‡ {report['individual_accuracies'][i]:.4f}, "
                       f"æ¬Šé‡ {report['model_weights'][i]:.4f}, "
                       f"éŒ¯èª¤ç‡ {report['model_errors'][i]:.4f}\n")


def create_data_loaders(args):
    """å‰µå»ºè³‡æ–™è¼‰å…¥å™¨"""
    if args.data == 'medicine':
        mean_values = [0.485, 0.456, 0.406]
        std_values = [0.229, 0.224, 0.225]
        transform = transforms.Compose([
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean_values, std=std_values)
        ])
        
        # é©—è­‰é›†
        val_dir = os.path.join('medicine_picture', 'valid')
        val_dataset = ImageFolder(val_dir, transform=transform, index=list(range(args.num_class)))
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0
        )
        
        # æ¸¬è©¦é›†
        test_dir = os.path.join('medicine_picture', 'test') if os.path.exists('medicine_picture/test') else val_dir
        test_dataset = ImageFolder(test_dir, transform=transform, index=list(range(args.num_class)))
        test_loader = torch.utils.data.DataLoader(
            test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0
        )
        
    elif 'cifar' in args.data:
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        # å‰µå»ºç›®æ¨™è½‰æ›ï¼ŒCIFAR100éœ€è¦é€™å€‹åƒæ•¸
        target_transform = np.arange(args.num_class)
        
        val_dataset = CIFAR100(root=args.data_dir, train=False, download=True, 
                              transform=transform, target_transform=target_transform,
                              index=list(range(args.num_class)))
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0
        )
        test_loader = val_loader
        
    return val_loader, test_loader


def main():
    parser = argparse.ArgumentParser(description='AdaBoost-Stacking Hybrid Ensemble Learning')
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('-data', type=str, default='medicine', help='è³‡æ–™é›†é¡å‹')
    parser.add_argument('-models_dir', type=str, required=True, help='æ¨¡å‹æª”æ¡ˆç›®éŒ„')
    parser.add_argument('-num_class', type=int, default=1000, help='é¡åˆ¥ç¸½æ•¸')
    parser.add_argument('-nb_cl_fg', type=int, default=500, help='ç¬¬ä¸€çµ„é¡åˆ¥æ•¸')
    parser.add_argument('-num_task', type=int, default=10, help='å¾ŒçºŒä»»å‹™æ•¸')
    parser.add_argument('-epochs', type=int, default=200, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('-batch_size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('-gpu', type=str, default='0', help='ä½¿ç”¨çš„GPU')
    parser.add_argument('-data_dir', type=str, default='/data/datasets/', help='è³‡æ–™ç›®éŒ„')
    parser.add_argument('-save_dir', type=str, default='AdaBoost_Stacking_Hybrid_Results', help='çµæœå„²å­˜ç›®éŒ„')
    
    args = parser.parse_args()
    
    # è¨­å®šè¨­å‚™
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # æ§‹å»ºæ¨¡å‹è·¯å¾‘å’Œé¡åˆ¥ç¯„åœ
    model_paths = []
    class_ranges = []
    
    # ç¬¬ä¸€å€‹æ¨¡å‹ï¼ˆä»»å‹™0ï¼Œé¡åˆ¥0-499ï¼‰
    model_paths.append(os.path.join(args.models_dir, f'task_00_{args.epochs-1}_model.pkl'))
    class_ranges.append((0, args.nb_cl_fg - 1))
    
    # å¾ŒçºŒæ¨¡å‹ï¼ˆä»»å‹™1-10ï¼Œæ¯å€‹50å€‹é¡åˆ¥ï¼‰
    num_class_per_task = (args.num_class - args.nb_cl_fg) // args.num_task
    for task_id in range(1, args.num_task + 1):
        model_paths.append(os.path.join(args.models_dir, f'task_{task_id:02d}_{args.epochs-1}_model.pkl'))
        start_class = args.nb_cl_fg + (task_id - 1) * num_class_per_task
        end_class = args.nb_cl_fg + task_id * num_class_per_task - 1
        class_ranges.append((start_class, end_class))
    
    print("ğŸ¯ AdaBoost-Stackingæ··åˆé›†æˆé…ç½®:")
    print("æ–¹æ³•ç‰¹å¾µ:")
    print("  âœ… å‰æœŸè™•ç†: Stackingæ–¹å¼ (æ•´é«”é©—è­‰é›†è©•ä¼°)")
    print("  âœ… æœ€çµ‚æ±ºç­–: AdaBoostæ–¹å¼ (åŠ æ¬ŠæŠ•ç¥¨)")
    print("  âœ… æ¬Šé‡è¨ˆç®—: AdaBoostå…¬å¼åŸºæ–¼æ•´é«”è¡¨ç¾")
    print("\næ¨¡å‹é…ç½®:")
    for i, (path, class_range) in enumerate(zip(model_paths, class_ranges)):
        print(f"æ¨¡å‹ {i+1:2d}: {os.path.basename(path)} -> é¡åˆ¥ {class_range[0]}-{class_range[1]}")
    
    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆå­˜åœ¨æ€§
    missing_models = [path for path in model_paths if not os.path.exists(path)]
    if missing_models:
        print("âŒ éŒ¯èª¤ï¼šä»¥ä¸‹æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼š")
        for path in missing_models:
            print(f"  - {path}")
        return
    
    # å‰µå»ºæ··åˆé›†æˆå™¨
    print("\nåˆå§‹åŒ–AdaBoost-Stackingæ··åˆé›†æˆå™¨...")
    ensemble = AdaBoostStackingHybrid(
        model_paths=model_paths,
        class_ranges=class_ranges,
        num_classes=args.num_class,
        device=device
    )
    
    # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
    print("æº–å‚™è³‡æ–™è¼‰å…¥å™¨...")
    val_loader, test_loader = create_data_loaders(args)
    
    # ä½¿ç”¨Stackingæ–¹å¼è¨ˆç®—æ¬Šé‡
    print("ä½¿ç”¨Stackingæ–¹å¼è¨ˆç®—AdaBoostæ¬Šé‡...")
    ensemble.compute_stacking_weights(val_loader)
    
    # ä½¿ç”¨AdaBoostæ–¹å¼é€²è¡Œé›†æˆé æ¸¬
    print("ä½¿ç”¨AdaBoostæ–¹å¼é€²è¡Œé›†æˆé æ¸¬...")
    results = ensemble.predict_ensemble(test_loader)
    
    # åˆ†æå’Œå„²å­˜çµæœ
    print("åˆ†ææ€§èƒ½çµæœ...")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = os.path.join(args.save_dir, f"run_{timestamp}")
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\nğŸ“ æ··åˆé›†æˆçµæœå°‡å„²å­˜æ–¼ï¼š")
    print(f"   {save_dir}")
    print(f"   â”œâ”€â”€ performance_report.json")
    print(f"   â”œâ”€â”€ performance_summary.txt") 
    print(f"   â”œâ”€â”€ overall_performance.png")
    print(f"   â”œâ”€â”€ model_weights.png")
    print(f"   â””â”€â”€ confusion_matrix.png")
    
    ensemble.analyze_performance(results, save_dir)
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "="*70)
    print("ğŸ¯ AdaBoost-Stacking æ··åˆé›†æˆå­¸ç¿’å®Œæˆï¼")
    print("="*70)
    print(f"æ–¹æ³•ç‰¹å¾µ: Stackingå‰æœŸè™•ç† + AdaBoostæœ€çµ‚æ±ºç­–")
    print(f"é›†æˆæº–ç¢ºç‡: {results['ensemble_accuracy']:.4f}")
    print(f"æœ€ä½³å€‹åˆ¥æ¨¡å‹: {max(results['individual_accuracies']):.4f}")
    print(f"æ”¹å–„å¹…åº¦: {results['ensemble_accuracy'] - max(results['individual_accuracies']):.4f}")
    print(f"çµæœå„²å­˜æ–¼: {save_dir}")
    print("="*70)


if __name__ == '__main__':
    main() 